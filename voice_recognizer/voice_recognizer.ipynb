{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "from torchaudio.datasets import SPEECHCOMMANDS\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningDataModule, LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "labels = ['backward', 'bed', 'bird', 'cat','dog', 'down', 'eight', 'five', 'follow', 'forward', 'four', 'go', 'happy', 'house', 'learn',\n",
    "          'left', 'marvin', 'nine', 'no', 'off', 'on', 'one', 'right', 'seven', 'sheila', 'six', 'stop', 'three', 'tree', 'two', 'up',\n",
    "          'visual', 'wow', 'yes', 'zero']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class SpeechCommands(SPEECHCOMMANDS):\n",
    "    def __init__(self, subset: str = None):\n",
    "        super().__init__(\"./\", download=True)\n",
    "\n",
    "        def load_list(filename):\n",
    "            filepath = os.path.join(self._path, filename)\n",
    "            with open(filepath) as fileobj:\n",
    "                return [os.path.normpath(os.path.join(self._path, line.strip())) for line in fileobj]\n",
    "\n",
    "        if subset == \"validate\":\n",
    "            self._walker = load_list(\"validation_list.txt\")\n",
    "        elif subset == \"test\":\n",
    "            self._walker = load_list(\"testing_list.txt\")\n",
    "        elif subset == \"train\":\n",
    "            excludes = load_list(\"validation_list.txt\") + load_list(\"testing_list.txt\")\n",
    "            excludes = set(excludes)\n",
    "            self._walker = [w for w in self._walker if w not in excludes]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def label_to_index(word):\n",
    "    return torch.tensor(labels.index(word))\n",
    "\n",
    "def index_to_label(index):\n",
    "    return labels[index]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class SpeechDataModule(LightningDataModule):\n",
    "    def __init__(self, batch_size, labels=labels, transform=None):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            self.train_set = SpeechCommands(\"train\")\n",
    "            self.valid_set = SpeechCommands(\"validate\")\n",
    "\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.test_set = SpeechCommands(\"test\")\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        loader = DataLoader(self.train_set,\n",
    "                            batch_size=self.batch_size,\n",
    "                            shuffle=True,\n",
    "                            drop_last=True,\n",
    "                            collate_fn=self.collate_fn,\n",
    "                            pin_memory=True,\n",
    "                            num_workers=4\n",
    "                           )\n",
    "        return loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        loader = DataLoader(self.valid_set,\n",
    "                            batch_size=self.batch_size,\n",
    "                            shuffle=False,\n",
    "                            drop_last=False,\n",
    "                            collate_fn=self.collate_fn,\n",
    "                            pin_memory=True,\n",
    "                            num_workers=4\n",
    "                           )\n",
    "        return loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        loader = DataLoader(self.test_set,\n",
    "                            batch_size=self.batch_size,\n",
    "                            shuffle=False,\n",
    "                            drop_last=False,\n",
    "                            collate_fn=self.collate_fn,\n",
    "                            pin_memory=True,\n",
    "                            num_workers=4\n",
    "                           )\n",
    "        return loader\n",
    "\n",
    "    def pad_sequence(self, batch):\n",
    "        batch = [item.t() for item in batch]\n",
    "        batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0.)\n",
    "        batch = self.transform(batch.permute(0, 2, 1))\n",
    "        return batch\n",
    "\n",
    "    def label_to_index(self, word):\n",
    "        return torch.tensor(self.labels.index(word))\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        tensors, targets = [], []\n",
    "\n",
    "        for waveform, _, label, _, _ in batch:\n",
    "            tensors += [waveform]\n",
    "            targets += [self.label_to_index(label)]\n",
    "\n",
    "        tensors = self.pad_sequence(tensors)\n",
    "        targets = torch.stack(targets)\n",
    "\n",
    "        return tensors, targets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "train_set = SpeechCommands(\"train\")\n",
    "test_set = SpeechCommands(\"test\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class VoiceRecognizerNN(nn.Module):\n",
    "    def __init__(self, n_input=1, n_output=35, stride=16, n_channel=32):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(n_input, n_channel, kernel_size=80, stride=stride)\n",
    "        self.bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.pool1 = nn.MaxPool1d(4)\n",
    "        self.conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.pool2 = nn.MaxPool1d(4)\n",
    "        self.conv3 = nn.Conv1d(n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.pool3 = nn.MaxPool1d(4)\n",
    "        self.conv4 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.pool4 = nn.MaxPool1d(4)\n",
    "        self.fc1 = nn.Linear(2 * n_channel, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.bn3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.bn4(x))\n",
    "        x = self.pool4(x)\n",
    "        x = F.avg_pool1d(x, x.shape[-1])\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim=2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class VoiceRecognizer(LightningModule):\n",
    "    def __init__(self, model_hparams, optimizer_hparams):\n",
    "        super().__init__()\n",
    "        self.optimizer_hparams = optimizer_hparams\n",
    "        self.model = VoiceRecognizerNN(model_hparams[\"n_input\"],\n",
    "                        model_hparams[\"n_output\"],\n",
    "                        model_hparams[\"stride\"],\n",
    "                        model_hparams[\"n_channel\"]\n",
    "                       )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return F.log_softmax(x, dim=2)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data, target = batch\n",
    "        output = self(data).squeeze()\n",
    "        loss = F.nll_loss(output, target)\n",
    "        self.log(\"tr_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        data, target = batch\n",
    "        output = self(data).squeeze()\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        preds = torch.argmax(output, dim=1)\n",
    "        acc = (target == preds).float().mean()\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        data, target = batch\n",
    "        output = self(data).squeeze()\n",
    "\n",
    "        preds = torch.argmax(output, dim=1)\n",
    "        acc = (target == preds).float().mean()\n",
    "        self.log(\"test_acc\", acc, prog_bar=True)\n",
    "        return acc\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        params = self.optimizer_hparams\n",
    "        optimizer = torch.optim.AdamW(self.parameters(),\n",
    "                                      lr=params[\"learning_rate\"],\n",
    "                                      weight_decay=params[\"weight_decay\"]\n",
    "                                     )\n",
    "\n",
    "        schedular = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                    gamma=params[\"gamma\"],\n",
    "                                                    step_size=params[\"step_size\"]\n",
    "                                                   )\n",
    "        scheduler = {\"scheduler\": schedular, \"interval\": \"epoch\", \"frequency\": 1}\n",
    "        return [optimizer], [scheduler]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "if device == \"cuda\":\n",
    "    num_workers = 1\n",
    "    pin_memory = True\n",
    "else:\n",
    "    num_workers = 0\n",
    "    pin_memory = False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "max_epochs = 0\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=0.00, patience=5, verbose=False, mode=\"min\")\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\", dirpath='./speech/models/', filename='spcm_{epoch:03d}', save_top_k=5)\n",
    "model_hparams = {\"n_input\": 1,\n",
    "          \"n_output\": 35,\n",
    "          \"stride\": 16,\n",
    "          \"n_channel\": 32\n",
    "         }\n",
    "optimizer_hparams = {\"learning_rate\": 0.01,\n",
    "                      \"weight_decay\": 0.0001,\n",
    "                      \"gamma\": 0.5,\n",
    "                      \"step_size\": 10,\n",
    "                      \"patience\": 5\n",
    "                     }\n",
    "model = VoiceRecognizer(model_hparams, optimizer_hparams)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(max_epochs=max_epochs,\n",
    "                  accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
    "                  precision=16,  # Automatic Mixed Precision (AMP)\n",
    "                  callbacks=[early_stop_callback, checkpoint_callback]\n",
    "                 )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "orig_freq = 16000\n",
    "new_freq = 8000\n",
    "transform = torchaudio.transforms.Resample(orig_freq=orig_freq, new_freq=new_freq)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "dm = SpeechDataModule(batch_size=batch_size, transform=transform)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pavle/Documents/MLAV/venv/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:604: UserWarning: Checkpoint directory /home/pavle/Documents/MLAV/voice_recognizer/speech/models exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type              | Params\n",
      "--------------------------------------------\n",
      "0 | model | VoiceRecognizerNN | 26.9 K\n",
      "--------------------------------------------\n",
      "26.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "26.9 K    Total params\n",
      "0.054     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b357f22d62fc485b89a8391204c9b0ee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=0` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, dm)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not import the PyAudio C module 'pyaudio._portaudio'.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "libportaudio.so.2: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpyaudio\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mwave\u001B[39;00m\n\u001B[1;32m      4\u001B[0m chunk \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1024\u001B[39m  \u001B[38;5;66;03m# Record in chunks of 1024 samples\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/MLAV/venv/lib/python3.10/site-packages/pyaudio/__init__.py:111\u001B[0m\n\u001B[1;32m    108\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mwarnings\u001B[39;00m\n\u001B[1;32m    110\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 111\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpyaudio\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_portaudio\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpa\u001B[39;00m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m:\n\u001B[1;32m    113\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not import the PyAudio C module \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpyaudio._portaudio\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mImportError\u001B[0m: libportaudio.so.2: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "\n",
    "chunk = 1024  # Record in chunks of 1024 samples\n",
    "sample_format = pyaudio.paInt16  # 16 bits per sample\n",
    "channels = 2\n",
    "fs = 44100  # Record at 44100 samples per second\n",
    "seconds = 3\n",
    "filename = \"output.wav\"\n",
    "\n",
    "p = pyaudio.PyAudio()  # Create an interface to PortAudio\n",
    "\n",
    "print('Recording')\n",
    "\n",
    "stream = p.open(format=sample_format,\n",
    "                channels=channels,\n",
    "                rate=fs,\n",
    "                frames_per_buffer=chunk,\n",
    "                input=True)\n",
    "\n",
    "frames = []  # Initialize array to store frames\n",
    "\n",
    "# Store data in chunks for 3 seconds\n",
    "for i in range(0, int(fs / chunk * seconds)):\n",
    "    data = stream.read(chunk)\n",
    "    frames.append(data)\n",
    "\n",
    "# Stop and close the stream\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "# Terminate the PortAudio interface\n",
    "p.terminate()\n",
    "\n",
    "print('Finished recording')\n",
    "\n",
    "# Save the recorded data as a WAV file\n",
    "wf = wave.open(filename, 'wb')\n",
    "wf.setnchannels(channels)\n",
    "wf.setsampwidth(p.get_sample_size(sample_format))\n",
    "wf.setframerate(fs)\n",
    "wf.writeframes(b''.join(frames))\n",
    "wf.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
